# ğŸªœ How Should We Break Down a Task? â€” Speaker Prompts + Examples

- **â€œWhatâ€™s wrong with a task like â€˜Build analytics dashboardâ€™?â€**  
  â†’ Itâ€™s too vague â€” doesnâ€™t help us estimate, test, or know when weâ€™re done.  
  ğŸ—£ *Example fallback:* â€œBetter: â€˜Build API to return active user count,â€™ â€˜Render line chart in admin panel.â€™â€

- **â€œHow do you define when a task is *done*?â€**  
  â†’ Should have a clear, testable output.  
  ğŸ—£ *Example:* â€œFor API: JSON returns expected fields, test added. For UI: chart renders sample data, layout works.â€

- **â€œWhy split tasks by delivery steps instead of tech layers?â€**  
  â†’ Helps deliver value earlier and reduce dependencies.  
  ğŸ—£ *Example:* â€œStep 1: API returns mock data. Step 2: UI renders mock chart. Step 3: Connect real data.â€

- **â€œWhen should we use a spike?â€**  
  â†’ When thereâ€™s too much uncertainty to estimate confidently.  
  ğŸ—£ *Example:* â€œIf weâ€™re not sure whatâ€™s in the logs or if we even track that metric â€” do a spike first.â€

- **â€œDo you create tasks for tests, docs, or cleanup?â€**  
  â†’ Yes, and you should â€” theyâ€™re real work, not optional.  
  ğŸ—£ *Example:* â€œWrite a separate task for unit tests, or for documenting how the dashboard works.â€

---

# ğŸ” Why Spike Tasks Matter â€” Speaker Prompts + Examples

- **â€œWhat is a spike task, and why do we need it?â€**  
  â†’ A spike is a time-boxed investigation â€” we use it to explore unknowns before estimating.  
  ğŸ—£ *Example:* â€œWe think we have usage data, but it's stored in messy `jsonb` logs. A spike helps us confirm what's usable.â€

- **â€œWhen should we create a spike?â€**  
  â†’ When a task has too many unknowns, or the risk of failure is high.  
  ğŸ—£ *Example:* â€œIf weâ€™re unsure whether we even log feature usage, donâ€™t guess â€” spike it.â€

- **â€œHow long should a spike take?â€**  
  â†’ It should be short and focused â€” usually 1 day or less.  
  ğŸ—£ *Example:* â€œWe time-boxed a 1-day spike to explore user activity data and came out with clear follow-up tasks.â€

- **â€œWhat should the outcome of a spike be?â€**  
  â†’ Itâ€™s not a deliverable â€” itâ€™s knowledge.  
  ğŸ—£ *Example:* â€œAfter the spike, we know whatâ€™s available, what needs cleanup, and what to estimate properly.â€

- **â€œWhat happens if we skip the spike?â€**  
  â†’ You risk underestimating and getting stuck mid-sprint.  
  ğŸ—£ *Example:* â€œWe once skipped it, assumed everything was there, then realized halfway through we had to write migration scripts.â€

---

# ğŸ¯ Why Points Can Fail in Small Teams â€” Speaker Prompts + Examples

- **â€œHave you ever used story points? Did they work well?â€**  
  â†’ In small teams, they often create confusion or inconsistency.  
  ğŸ—£ *Example:* â€œWe said this was a â€˜3-pointer,â€™ but no one agreed what 3 points meant.â€

- **â€œWhy do story points break down in small teams?â€**  
  â†’ Not enough historical data, and small team velocity fluctuates a lot.  
  ğŸ—£ *Example:* â€œOne person out sick can drop our â€˜velocityâ€™ by 30% â€” thatâ€™s noise, not insight.â€

- **â€œWhatâ€™s the risk of using points without shared context?â€**  
  â†’ People estimate based on hours, others on complexity â€” no common ground.  
  ğŸ—£ *Example:* â€œSomeone thought 5 points = 1 week, someone else thought it was a risky 2-day task.â€

- **â€œWhat can we do instead?â€**  
  â†’ Use hours or day-sized chunks if that feels more natural.  
  ğŸ—£ *Example:* â€œWe started using half-day / full-day sizing and focused more on task clarity than point numbers.â€

- **â€œIf you still want to use points, how do you make them work?â€**  
  â†’ Create shared examples and anchor tasks.  
  ğŸ—£ *Example:* â€œWe wrote down: â€˜This login task is a 2-pointer,â€™ so we had a real reference to compare new tasks.â€

---

# ğŸ”§ How Should We Code It? â€” Speaker Prompts + Examples

- **â€œWhatâ€™s the first thing you do when implementing a feature?â€**  
  â†’ Start with refactoring or reviewing the existing code.  
  ğŸ—£ *Example:* â€œBefore building the usage API, we cleaned up the old metrics script and added basic tests â€” it helped uncover edge cases early.â€

- **â€œWhy refactor first?â€**  
  â†’ It helps you understand the system, reduce bugs, and add guardrails.  
  ğŸ—£ *Example:* â€œYou canâ€™t build clean code on top of a mess â€” refactoring first makes your job easier, not harder.â€

- **â€œDo we need a design spec or doc for every task?â€**  
  â†’ Not always, but for complex features, a 1-pager helps align the team.  
  ğŸ—£ *Example:* â€œWe sketched the dashboard API + UI on a doc before writing any code â€” it saved us 2â€“3 rounds of rework.â€

- **â€œWhat about documentation?â€**  
  â†’ Small teams donâ€™t need heavy docs â€” make the code clear instead.  
  ğŸ—£ *Example:* â€œUse good naming, comments where it matters, and keep functions focused â€” your future self will thank you.â€

- **â€œWhy invest in automated tests?â€**  
  â†’ They catch regressions and save hours during refactors.  
  ğŸ—£ *Example:* â€œWe added a test for usage aggregation â€” caught a bug the next day when someone changed the query.â€

---

# ğŸŒ€ How Do We Avoid Mid-Sprint Randomness? â€” Speaker Prompts + Examples

- **â€œWhat kind of things have derailed your sprint before?â€**  
  â†’ Unexpected bug, urgent sales ask, vague task that turns out huge  
  ğŸ—£ *Example:* â€œWe once added â€˜small metrics tweakâ€™ mid-sprint â€” turned out to need new infra setup and blew up 2 days.â€

- **â€œHow do we avoid this kind of disruption?â€**

  - **ğŸ“¦ Keep the backlog tight**  
    â†’ Only add well-defined tasks to the sprint  
    ğŸ—£ *Example:* â€œWe used to drag in half-baked ideas â€” now we only sprint whatâ€™s clarified.â€

  - **ğŸ“£ Communicate early**  
    â†’ If something feels wrong or bigger than expected, say so early  
    ğŸ—£ *Example:* â€œSomeone flagged a vague UI task on day 1 â€” we paused and rewrote it before wasting time.â€

  - **ğŸš« Donâ€™t bundle tech debt into feature work unless planned**  
    â†’ Itâ€™s tempting, but unplanned refactors can throw off timelines  
    ğŸ—£ *Example:* â€œWe scoped feature work + refactor together, and it worked. When we didnâ€™t, it always overflowed.â€

  - **ğŸ§¯ Escalate scope creep early**  
    â†’ Adjust the plan instead of powering through  
    ğŸ—£ *Example:* â€œMid-sprint, we realized the dashboard needed role-based access. We split that into next sprint and shipped what we could.â€
