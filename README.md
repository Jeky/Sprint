---
marp: true
theme: default
class: lead
---

# ğŸ§­ Product Context

- SaaS product live for 1 year
- Customers request visibility into **team usage**
- Need a **dashboard for org admins**

**Requested metrics:**
- Active users over time
- Most used features
- Last login per user
- Org-wide usage summary

---

# ğŸ§± Current Codebase Reality

- Backend logs activity in `jsonb`, semi-structured
- Internal analytics exist but not reusable
- Frontend has an admin panel, but itâ€™s underdeveloped
- Mixed code quality:
  - User/auth logic is clean
  - Analytics code is messy and scattered
  - Minimal test coverage in this area
  - No design system for frontend

---

# ğŸ§  How Do We Estimate the Task?

1. ğŸªœ How should we break down a task?
2. ğŸ“ How should we estimate it?
3. ğŸ”§ How should we code it? (Implementation lifecycle)
4. ğŸŒ€ How do we avoid mid-sprint randomness?

---

# ğŸªœ How Should We Break Down a Task?

**Goal:** Make tasks small, focused, and estimate-friendly.

### ğŸ”¹ Good Practices

- âŒ Avoid vague tasks â€” be specific and outcome-focused  
- âœ… Define â€œdoneâ€ for every task  
- ğŸ”„ Split work by delivery steps, not tech layers  
- ğŸ” Use spike tasks for investigation or unclear work  
- ğŸ§¼ Include supporting tasks (tests, docs, cleanup)

---

# ğŸ“ How Should We Estimate a Task?

**Goal:** Estimate with enough accuracy to plan, not to promise.

### ğŸ”¹ Good Practices

- ğŸ§© Break down before estimating  
- ğŸ”„ Compare with similar past work  
- ğŸ¤ Estimate together, not solo  
- â“ Call out unknowns or risks  
- ğŸ“Š Use points, hours, or sizes â€” just be consistent

---

# ğŸ” Why Spike Tasks Matter

**Spikes = Time-boxed investigation tasks**  
Used to explore or reduce uncertainty before estimation.

### ğŸ’¥ When to Use Them

- Task has too many unknowns to estimate confidently  
- Need to assess feasibility or options  
- Risk of under/over-estimation is high

### âœ… Benefits

- De-risk upcoming work  
- Build shared understanding  
- Allow better estimation next sprint  
- Prevent random deep-dives mid-sprint

---

# ğŸ¯ Why Points Can Fail in Small Teams

Story points are often **too abstract** for small teams, especially without consistent velocity data.

### âš ï¸ Common Issues

- Not enough historical data to calibrate  
- Points become guesses, not comparisons  
- People confuse points with hours  
- Too few people â†’ team velocity is volatile

### âœ… What to Do Instead

- Use **hours or day-size chunks** if you prefer concreteness  
- Focus on **task size + clarity**, not exact number  
- If using points, **build shared examples** (e.g. â€œThis is a 2-pointerâ€)  

---

# ğŸ”§ How Should We Code It?

**Goal:** Deliver high-quality features with minimal friction.

### ğŸ”¹ Good Practices

- ğŸ§¼ Start with refactoring â€” helps understand the code and add safety (tests)
- ğŸ§  Align early with a 1-pager design for complex features
- ğŸ§¾ Avoid over-documenting â€” write self-explanatory code instead
- âœ… Invest in automated tests â€” they save hours later

---

# ğŸŒ€ How Do We Avoid Mid-Sprint Randomness?

**Goal:** Stay focused on what we committed, while being realistic.

### ğŸ”¹ Strategies That Help

- ğŸ“¦ Keep the sprint backlog tight â€” only what's well-defined and ready
- ğŸ“£ Communicate â€” speak up early when something goes off track
- ğŸš« Avoid bundling tech debt into feature work unless planned
- ğŸ§¯ Escalate scope creep early â€” replan if needed
